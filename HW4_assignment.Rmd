---
title: "Homework 4"
author: Jeong lim Kim, Jayoung Kang, Hye-Min Jung
date: "5/5/2020"
fontsize: 10 pt
output:
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      #include = TRUE, 
                      fig.width = 6, fig.height = 4,
                      results='markup',
                      warning = FALSE,
                      cache = TRUE,
                      digits = 3,
                      width = 48) 
```

```{r, include=FALSE}
## microfinance network 
## data from BANERJEE, CHANDRASEKHAR, DUFLO, JACKSON 2012

## data on 8622 households
hh <- read.csv("microfi_households.csv", row.names="hh")
hh$village <- factor(hh$village)

## We'll kick off with a bunch of network stuff.
## This will be covered in more detail in lecture 6.
## get igraph off of CRAN if you don't have it
## install.packages("igraph")
## this is a tool for network analysis
## (see http://igraph.sourceforge.net/)
library(igraph)
edges <- read.table("microfi_edges.txt", colClasses="character")
## edges holds connections between the household ids
hhnet <- graph.edgelist(as.matrix(edges))
hhnet <- as.undirected(hhnet) # two-way connections.

## igraph is all about plotting.  
V(hhnet) ## our 8000+ household vertices
## Each vertex (node) has some attributes, and we can add more.
V(hhnet)$village <- as.character(hh[V(hhnet),'village'])
## we'll color them by village membership
vilcol <- rainbow(nlevels(hh$village))
names(vilcol) <- levels(hh$village)
V(hhnet)$color = vilcol[V(hhnet)$village]
## drop HH labels from plot
V(hhnet)$label=NA

# graph plots try to force distances proportional to connectivity
# imagine nodes connected by elastic bands that you are pulling apart
# The graphs can take a very long time, but I've found
# edge.curved=FALSE speeds things up a lot.  Not sure why.

## we'll use induced.subgraph and plot a couple villages 
village1 <- induced.subgraph(hhnet, v=which(V(hhnet)$village=="1"))
village33 <- induced.subgraph(hhnet, v=which(V(hhnet)$village=="33"))

# vertex.size=3 is small.  default is 15
plot(village1, vertex.size=3, edge.curved=FALSE)
plot(village33, vertex.size=3, edge.curved=FALSE)
```

```{r, include=FALSE}
######  now, on to your homework stuff

library(gamlr)

## match id's; I call these 'zebras' because they are like crosswalks
zebra <- match(rownames(hh), V(hhnet)$name)

## calculate the `degree' of each hh: 
##  number of commerce/friend/family connections
degree <- degree(hhnet)[zebra]
names(degree) <- rownames(hh)
degree[is.na(degree)] <- 0 # unconnected houses, not in our graph

## if you run a full glm, it takes forever and is an overfit mess
# summary(full <- glm(loan ~ degree + .^2, data=hh, family="binomial"))
# Warning messages:
# 1: glm.fit: algorithm did not converge 
# 2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
```

### [1] Iâ€™d transform degree to create our treatment variable d. 
* What would you do?
  + I would apply log(x+1) transformation to degree. 
* Why would you do that?
  + In this way, I can normalize distribution, resolving the skewness in the degree data. The distribution of d becomes closer to the normal gaussian distribution and suitable for fitting linear regression. 
  + Instead of simple log transformation, log(x + 1) is used for values that contain 0. This enable us to avoid log transformation returning `-Inf`s.
```{r}
degree <- as.data.frame(degree)
hist(degree$degree, main = "Histogram of degree")

degree$degree <- log(degree$degree+1) # transform relevant variables with log(variable + 1)
degree$degree[is.na(degree)] <- 0 # remove nans

hist(degree$degree, main = "Histogram of log(degree)") # normalized distribution
hh <- cbind(hh,degree)
```

## [2] Build a model to predict d from x, our controls. 
* Comment on how tight the fit of model built? 
  + Model fit(R square) is really large, 0.0819.

* What does fit implies for estimation of a treatment effect?
  + This implies large confounding effect. In other words, there is not much additional information in the treatment on top of Xs, because there are so much overwrapping part between treatment effect and X. 
```{r}
## double LASSO preparation
y <- hh$loan #outcome
d <- hh$degree #treatment
x = sparse.model.matrix(~ . -loan -degree , data=hh)[,-1] #confounders
dim(x)

# FIRST STAGE:
# do LASSO of treatment on confounders

treat <- gamlr(x,d,lambda.min.ratio=1e-4)
plot(treat) # there are some x's predictive of trestment

# Now, grab the predicted treatment
# type="response" is redundant here (gaussian), 
# but you'd want it if d was binary

# we isolate dhat (the part of treatment that we can predict with x's)
dhat <- predict(treat, x, type="response") 

## not much signal in d not predicted by dhat
plot(dhat,d,bty="n",pch=21,bg=8) 

# it seems that abortion does not have much extra information on top of x

## IS R^2?
cor(drop(dhat),d)^2

## Note: IS R2 is what governs how much independent signal
## you have for estimating 
```

## [3] Use predictions from [2] in an estimator for effect of d on loan.
* Coefficient for treatment is 0.0179. 
```{r}
# SECOND STAGE: 

# do lasso of outcome (loan) on treatment (d), predicted treatment (dhat) and predictors x
causal <- gamlr(cBind(d,dhat,x),y,free=2,lmr=1e-4)

# free=2 is needed because we need to make sure that dhat always stays in the model, i.e. we free it from LASSO shrinkage
# dhat separates d from x, now the effect of d will be "pure" (isolated from x)

coef(causal)["d",] # AICc says abortion has no causal effect.

# no extra effect it turns out
```

## [4] Compare the results from [3] to those from a straight (naive) lasso for loan on d and x. Explain why they are similar or different.
* Naive Lasso coefficient is also 0.0187 
* Why similar? ????
```{r}
## NAIVE LASSO regression

# Naive LASSO adds "treatment" as an extra covariate without giving it any special attention
naive <- gamlr(cBind(d,x),y)
coef(naive)["d",] # effect is AICc selected <0

# this is the effect of treatment (abortion), given everything else that LASSO keeps in the model
# the "everything else", however, might not include all the confounders :(
```

## [5] Bootstrap your estimator from [3] and describe the uncertainty.
* All those replicates, with estimated 0 treatment effect, standard error turns out to be 0.
* Here we have no uncertainty, with the evidence overwhelmingly in favor of having the treatment effect to be zero. 
  + If the treatment effect was non zero and then we could have gotten more variability. 
```{r}
## BOOTSTRAP 

n <- nrow(x)

## Bootstrapping our lasso causal estimator is easy

gamb <- c() # empty gamma

for(b in 1:20){
	## create a matrix of resampled indices

	ib <- sample(1:n, n, replace=TRUE)

	## create the resampled data

	xb <- x[ib,]

	db <- d[ib]

	yb <- y[ib]

	## run the treatment regression

	treatb <- gamlr(xb,db,lambda.min.ratio=1e-3)

	dhatb <- predict(treatb, xb, type="response")

	fitb <- gamlr(cBind(db,dhatb,xb),yb,free=2)

	gamb <- c(gamb,coef(fitb)["db",])

	print(b)
}

## not very exciting though: all zeros

summary(gamb) 
``` 